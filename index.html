<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DH-Bench: Probing Depth and Height Perception of
  Large Visual-Language Models">
 
  <meta name="keywords" content="Visual language models, depth and height perception, benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DH-Bench: Probing Depth and Height Perception of
    Large Visual-Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/img5-copy-2.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DH-Bench: Probing Depth and Height Perception of
    Large Visual-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- put shhereen link -->
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Shehreen Azad</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="http://www.yash-jain.com" target="_blank">Yash Jain</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <!-- put rishit link -->
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Rishit Garg</a><sup>3</sup>,
                  </span>
                <span class="author-block">
                    <a href="https://www.crcv.ucf.edu/person/rawat/" target="_blank">Yogesh S Rawat</a><sup>1</sup>,
                  </span>
                <span class="author-block">
                    <a href="https://vibhav-vineet.github.io" target="_blank">Vibhav Vineet</a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Center for Research in Computer Vision, University of Central Florida; <sup>2</sup> Microsoft Research; <sup>3</sup> Indian Institute of Technology, Kharagpur.
<!--                       <br>Conferance name and year</span> -->
<!--                     <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2408.11748" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/sacrcv/DH-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2408.11748" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser img-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">What is DH-Bench?</h2>
      <h2 class="subtitle has-text-centered">
        <span style="font-weight:bold;">DH-Bench  </span>
        is a benchmark containing programmatically generated synthetic and real-world data for depth and height perception tasks that can be solved by humans very easily, but pose significant challenges for current visual language models (VLMs).</h2>
      <img src="static/images/Geom_Bench_image_stuff-23.png" height="100%"/>
      <h2 class="hero-body has-text-centered">
        <br>
        Example images in <span style="font-weight:bold;">DH-Bench</span>. Here each samples are shown with random
        query attributes- color and numeric label for Synthetic 2D, color and material for Synthetic 3D and
        numeric label for Real-World dataset. 
      </h2>
    <!-- </div>
  </div>
  <div class="container is-max-desktop">
    <div class="hero-body"> -->
      <img src="static/images/Geom_Bench_image_stuff-21.png" height="100%"/>
      <h2 class="hero-body has-text-centered">
        <br>
        Example image, prompt, its different question types, and corresponding answers in <span style="font-weight:bold;">DH-Bench</span>. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser img -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">This website is under construction. Thank you for your patience</h2>
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- start of under construction notice -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Geometric understanding is crucial for navigating and interacting with our environment.
            While large Vision Language Models (VLMs) demonstrate impressive
            capabilities, deploying them in real-world scenarios necessitates a comparable geometric
            understanding in visual perception. In this work, we focus on the geometric
            comprehension of these models; specifically targeting the depths and heights of
            objects within a scene. Our observations reveal that, although VLMs excel in basic
            geometric properties perception such as shape and size, they encounter significant
            challenges in reasoning about the depth and height of objects. To address this, we
            introduce a suite of benchmark datasets—encompassing Synthetic 2D, Synthetic
            3D, and Real-World scenarios—to rigorously evaluate these aspects. We benchmark
            17 state-of-the-art VLMs using these datasets and find that they consistently
            struggle with both depth and height perception. Our key insights include detailed
            analyses of the shortcomings in depth and height reasoning capabilities of VLMs
            and the inherent bias present in these models. This study aims to pave the way
            for the development of VLMs with enhanced geometric understanding, crucial for
            real-world applications.          
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         Your image here -->
        <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> --> 
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{azad2024dhbenchprobingdepthheight,
        title={DH-Bench: Probing Depth and Height Perception of Large Visual-Language Models}, 
        author={Shehreen Azad and Yash Jain and Rishit Garg and Yogesh S Rawat and Vibhav Vineet},
        year={2024},
        eprint={2408.11748},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2408.11748}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
